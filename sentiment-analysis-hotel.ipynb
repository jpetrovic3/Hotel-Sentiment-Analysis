{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport numpy as np\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport pandas as pd\nimport nltk\nimport random\nnltk.download('punkt')","metadata":{"execution":{"iopub.status.busy":"2022-11-14T07:52:05.980940Z","iopub.execute_input":"2022-11-14T07:52:05.981429Z","iopub.status.idle":"2022-11-14T07:52:12.355607Z","shell.execute_reply.started":"2022-11-14T07:52:05.981352Z","shell.execute_reply":"2022-11-14T07:52:12.353812Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file = pd.read_csv(\"../input/515k-hotel-reviews-data-in-europe/Hotel_Reviews.csv\")\nfile.head()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T07:52:26.836405Z","iopub.execute_input":"2022-11-14T07:52:26.837070Z","iopub.status.idle":"2022-11-14T07:52:34.486229Z","shell.execute_reply.started":"2022-11-14T07:52:26.837027Z","shell.execute_reply":"2022-11-14T07:52:34.485158Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nnew_file_ = file.drop([\"Average_Score\",\"Additional_Number_of_Scoring\",\"Review_Date\",\"Hotel_Name\",\"Reviewer_Nationality\",\"Total_Number_of_Reviews\",\"Total_Number_of_Reviews_Reviewer_Has_Given\",\"Tags\",\"days_since_review\",\"lat\",\"lng\" ],axis = 1)\nnew_file = new_file_.loc[0:20000]\ntest_file = new_file_.loc[0:20500]\n\nrandom_size = len(new_file)/4\n\n\nprint(len(new_file))","metadata":{"execution":{"iopub.status.busy":"2022-11-14T07:52:37.451891Z","iopub.execute_input":"2022-11-14T07:52:37.452262Z","iopub.status.idle":"2022-11-14T07:52:37.502162Z","shell.execute_reply.started":"2022-11-14T07:52:37.452231Z","shell.execute_reply":"2022-11-14T07:52:37.500959Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data = []\ndata_test = []\nall_text = \"\"\nword2idx = {}\nidx2word = {}\n\n\nkey = 0\n\nword2idx.update({\"<end>\":key})\nidx2word.update({key:\"<end>\"})\n\nkey = 1\n\nmax_ = 0\nsize = [] \n\nprint(len(test_file))\n\n#combine negative and positive data\n\nfor i in range(0,len(test_file)):\n    if(i < len(new_file)):\n        size = nltk.word_tokenize(new_file[\"Negative_Review\"][i] + new_file[\"Positive_Review\"][i])\n        data.append(( nltk.word_tokenize(new_file[\"Negative_Review\"][i] + new_file[\"Positive_Review\"][i]), round(new_file[\"Reviewer_Score\"][i])))\n    else:\n        data_test.append(( nltk.word_tokenize(test_file[\"Negative_Review\"][i] + test_file[\"Positive_Review\"][i]), round(test_file[\"Reviewer_Score\"][i])))\n        size =  nltk.word_tokenize(test_file[\"Negative_Review\"][i] + test_file[\"Positive_Review\"][i])\n        \n    all_text += test_file[\"Negative_Review\"][i] + test_file[\"Positive_Review\"][i]\n    \n    if(max_ < len(size)):\n        max_ = len(size)\n    \n\ntext = nltk.word_tokenize(all_text)\n\n\n#create a vocab dictionary\nfor index, word in enumerate(text):\n    if word not in word2idx:\n        word2idx.update({word:key})\n        idx2word.update({key:word})\n        key += 1\n             \nprint(max_)\n\nvocab_size = len(word2idx)\nembbed_size = 10\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-14T07:53:08.721137Z","iopub.execute_input":"2022-11-14T07:53:08.721830Z","iopub.status.idle":"2022-11-14T07:53:21.351024Z","shell.execute_reply.started":"2022-11-14T07:53:08.721792Z","shell.execute_reply":"2022-11-14T07:53:21.349779Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"negative = 0\npositive = 0\n\nfor key, rev in enumerate(data):            \n    if(rev[1] < 5):\n        negative += 1\n    else:\n        rand = random.randrange(0,int(random_size))\n        if rand != 1:\n            data.pop(key)\n            \nfor key, rev in enumerate(data):\n    if(rev[1] < 5):\n        negative += 1\n    \n    else:\n        positive += 1\n        \n \nprint(negative)\nprint(positive)\n\nprint(len(data))\nratio = negative/len(data) * 100\nprint(ratio)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T07:53:26.942540Z","iopub.execute_input":"2022-11-14T07:53:26.943024Z","iopub.status.idle":"2022-11-14T07:53:27.055382Z","shell.execute_reply.started":"2022-11-14T07:53:26.942980Z","shell.execute_reply":"2022-11-14T07:53:27.054096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class classifier(nn.Module):\n    \n    def __init__(self,vocab_size, embbed_size):\n        super(classifier, self).__init__()\n        \n        self.vocab_size = vocab_size\n        self.embbed_size = embbed_size\n        \n        self.embbed = nn.Embedding(vocab_size, embbed_size)\n        self.rnn = nn.LSTM(embbed_size,10*embbed_size,2,True,False,0.2,True)\n        self.linear1 = nn.Linear(2*10*embbed_size*max_, 10)\n        self.softmax = nn.LogSoftmax(dim=1)\n       \n    def forward(self,input_):\n        \n        embeded_input = self.embbed(input_)\n        embeded_input = torch.unsqueeze(embeded_input, dim = 1)\n        output, (h,m) = self.rnn(embeded_input)\n        in_ = output.view((1,-1))\n        \n        out = self.linear1(in_)\n        out = self.softmax(out)\n        \n        return out \n\n        \n        ","metadata":{"execution":{"iopub.status.busy":"2022-11-14T07:53:31.633152Z","iopub.execute_input":"2022-11-14T07:53:31.633816Z","iopub.status.idle":"2022-11-14T07:53:31.641944Z","shell.execute_reply.started":"2022-11-14T07:53:31.633779Z","shell.execute_reply":"2022-11-14T07:53:31.640870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = classifier(vocab_size, embbed_size)\noptimizer = optim.Adam(model.parameters())\nloss_function  = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T07:53:35.225835Z","iopub.execute_input":"2022-11-14T07:53:35.226664Z","iopub.status.idle":"2022-11-14T07:53:35.252073Z","shell.execute_reply.started":"2022-11-14T07:53:35.226626Z","shell.execute_reply":"2022-11-14T07:53:35.251089Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def accuracy():\n    \n    model.eval()\n\n    with torch.no_grad():\n        \n        generous_correct  = 0\n        correct  = 0\n        for key, review in enumerate(data_test):\n\n            try:\n                input_ = torch.zeros(max_,dtype = int)\n\n                for index, i in enumerate(review[0]):\n                    input_[index] = word2idx[i]     \n\n                target =  torch.tensor([review[1] -1])\n\n                out = model(input_)\n\n                result = torch.argmax(out, dim = 1) \n\n\n                if(target == result):\n                    generous_correct += 1\n                    correct += 1\n                elif(target == result + 1):\n                    generous_correct += 1\n                elif(target == result - 1):\n                    generous_correct += 1\n\n            except:\n\n                continue \n\n        ratio = (correct/len(data_test)) * 100\n        ratio_ = (generous_correct/len(data_test)) * 100\n\n        print(\"correct \", ratio)\n        print(\"error correct \", ratio_)\n        \n        model.train()\n","metadata":{"execution":{"iopub.status.busy":"2022-11-14T07:53:38.130585Z","iopub.execute_input":"2022-11-14T07:53:38.130991Z","iopub.status.idle":"2022-11-14T07:53:38.141335Z","shell.execute_reply.started":"2022-11-14T07:53:38.130957Z","shell.execute_reply":"2022-11-14T07:53:38.140303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"correct = 0\nloss_value  = 0\n\nfor j in range(5):\n    \n    print(\"epoch \", j+1)\n    running_loss = 0\n    \n    for key, review in enumerate(data):\n        \n        model.zero_grad()\n        \n        try:\n            input_ = torch.zeros(max_, dtype = int)\n\n            for index, i in enumerate(review[0]):\n                input_[index] = word2idx[i]   \n\n            target =  torch.tensor([review[1] -1])\n            \n            out = model(input_)\n            loss = loss_function(out, target)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n                \n            if key % 2000 == 1999:    # print every 2000 mini-batches\n                print(f'[{j + 1}, {key + 1:5d}] loss: {running_loss / 100:.3f}')\n                running_loss = 0.0\n                accuracy()\n\n\n        except:\n            \n            print(\"word not in dictionary\")\n            continue \n        \n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy()","metadata":{"execution":{"iopub.status.busy":"2022-11-14T07:53:51.152071Z","iopub.execute_input":"2022-11-14T07:53:51.152459Z","iopub.status.idle":"2022-11-14T07:55:26.397985Z","shell.execute_reply.started":"2022-11-14T07:53:51.152425Z","shell.execute_reply":"2022-11-14T07:55:26.396119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"PATH = \"state_dict_model.pt\"\ntorch.save(model.state_dict(), PATH)","metadata":{"execution":{"iopub.status.busy":"2022-11-14T07:53:47.074924Z","iopub.execute_input":"2022-11-14T07:53:47.075318Z","iopub.status.idle":"2022-11-14T07:53:47.099111Z","shell.execute_reply.started":"2022-11-14T07:53:47.075267Z","shell.execute_reply":"2022-11-14T07:53:47.098126Z"},"trusted":true},"execution_count":null,"outputs":[]}]}
